{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d64ec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopogneri/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/jacopogneri/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583e6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [\"comments-user-toxicity-2020-04\", \"comments-user-toxicity-2020-05\", \"comments-user-toxicity-2020-06\", \"comments-user-toxicity-2020-07\", \"comments-user-toxicity-2020-08\", \"comments-user-toxicity-2020-09\", \"comments-user-toxicity-2020-10\", \"comments-user-toxicity-2020-11\", \"comments-user-toxicity-2020-12\", \"comments-user-toxicity-2021-01\"] \n",
    "folder = \"../JSON_comments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f637e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 4min 5s, total: 7min 2s\n",
      "Wall time: 8min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lines_def = []\n",
    "for i in range(0, len(json_files)):\n",
    "    with open(folder+json_files[i], 'r') as incsv:\n",
    "        for line in incsv:\n",
    "            lines_def.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d326cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8766596"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c100b",
   "metadata": {},
   "source": [
    "# emoji analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a671bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7e8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract emoji information (number, set and list of emojis)\n",
    "def extract_emojis(listLine):\n",
    "    for i in listLine:\n",
    "        count_e = emoji.emoji_count(i[\"text\"])\n",
    "        #if count_e != 0:\n",
    "         #   print(count_e)\n",
    "        set_e = ''.join(emoji.distinct_emoji_list(i[\"text\"]))\n",
    "        data_list_e = emoji.emoji_list(i[\"text\"])\n",
    "        if data_list_e:\n",
    "            str_e = ''.join([entry['emoji'] for entry in data_list_e])\n",
    "        else:\n",
    "            str_e = ''\n",
    "\n",
    "        standard_s = emojis.decode(i[\"text\"]) # the emojis are decoded (e.g. the smiley face becomes :smile:)\n",
    "        #standard_s.replace(':','') #used to remove the colons\n",
    "        i[\"text\"] = standard_s\n",
    "        i[\"emoji_count\"] = count_e\n",
    "        i[\"emoji_unique\"] = set_e\n",
    "        i[\"emoji_list\"] = str_e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613efdd",
   "metadata": {},
   "source": [
    "# Time handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1081a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece1d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes the key time, converts it into date and time_of day, adds these two keys and then removes the key time\n",
    "def convert_time(listLine):\n",
    "    for i in listLine:\n",
    "        date = datetime.fromtimestamp(i[\"time\"])\n",
    "        i[\"date\"] = date.date().strftime(\"%Y-%m-%d\")\n",
    "        i[\"time_of_day\"] = date.time().strftime(\"%H:%M:%S\")\n",
    "        del i[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d53e9",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599c682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "import re\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690e02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean a single text\n",
    "def clean_single_text(listLine):\n",
    "    for i in listLine:\n",
    "      # Decode HTML entities\n",
    "      i[\"text\"] = unescape(i[\"text\"])\n",
    "\n",
    "      # Remove URLs\n",
    "      i[\"text\"] = re.sub(r'https?://\\S+|www\\.\\S+', '', i[\"text\"])\n",
    "\n",
    "      # Remove symbols excluding numbers and punctuation\n",
    "      i[\"text\"] = re.sub(r'[^a-zA-Z0-9\\s' + re.escape(string.punctuation) + ']', '', i[\"text\"])\n",
    "\n",
    "      # Reduce multiple spaces to one\n",
    "      i[\"text\"] = re.sub(r'\\s+', ' ', i[\"text\"])\n",
    "\n",
    "      # Remove new lines and tabs\n",
    "      i[\"text\"] = re.sub(r'[\\n\\t]', ' ', i[\"text\"])\n",
    "\n",
    "      i[\"text\"] = i[\"text\"].strip()  # Remove leading and trailing spaces\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7038e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to expand contraction (it's => it is). slang is set to True as it is possible to find slang words in reddit posts\n",
    "#VERY FAST!\n",
    "def expand_contractions(listLine):\n",
    "    for i in listLine:\n",
    "        i[\"text\"] = contractions.fix(i[\"text\"], slang=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba93803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_upper_words(listLine):\n",
    "    for i in listLine:\n",
    "        # number of unique words in the text\n",
    "        i[\"num_unique_words\"] = len(set(str(i['text']).lower().split()))\n",
    "\n",
    "        # number of Upper case words in the text\n",
    "        i[\"num_words_upper\"] = len([w for w in str(i[\"text\"]).split() if w.isupper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f10054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_punct = ['\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n",
    "           '/', ':', ';', '<', '=', '>', '@', '[', '\\\\', ']', '^', '_',\n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', '#', '!', '?','.',':']\n",
    "\n",
    "punct_pattern = re.compile(\"[\" + re.escape(\"\".join(my_punct)) + \"]\")\n",
    "#function that removes punctuation\n",
    "def remove_punct(listLine):\n",
    "    for i in listLine:\n",
    "        i[\"text\"] = re.sub(punct_pattern, ' ', i[\"text\"])\n",
    "        i[\"text\"] = i[\"text\"].lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b6b6",
   "metadata": {},
   "source": [
    "# PosTag (non usa spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "045bea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jacopogneri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jacopogneri/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1dbf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS Tagging is useful to determine the number of \"full words\" (Verbs, Nouns and Adj) in a post\n",
    "def posTag(listLine):\n",
    "    for i in listLine:\n",
    "        tokens = word_tokenize(i[\"text\"])\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        # Counting the full words (Adjectives, Nouns, Verbs)\n",
    "        number_full_words = sum(1 for _, tag in pos_tags if tag.startswith((\"JJ\", \"NN\", \"VB\")))\n",
    "        i[\"number_full_words\"] = number_full_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e6a93",
   "metadata": {},
   "source": [
    "# NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65a0be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd94ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract emotion vectors using NRCLex\n",
    "def get_emotion_vector(listLine):\n",
    "    for i in listLine:\n",
    "        emotion = NRCLex(i[\"text\"])\n",
    "        affect_frequencies_dict = {emotion_class: round(frequency, 2) for emotion_class, frequency in emotion.affect_frequencies.items()}\n",
    "        for k, v in affect_frequencies_dict.items():\n",
    "            i[k] = v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d46de",
   "metadata": {},
   "source": [
    "## Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23232f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Readability\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30b30c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_tests(listLine):\n",
    "    for i in listLine:\n",
    "        text_str = ''.join(i[\"text\"])\n",
    "        i[\"flesch_kincaid\"] = 0\n",
    "        i[\"flesch\"] = 0\n",
    "        i[\"fog\"] = 0\n",
    "        i[\"coleman_liau\"] = 0\n",
    "        i[\"dale_chall\"] = 0\n",
    "        i[\"ari\"] = 0\n",
    "        i[\"linsear_write\"] = 0\n",
    "        i[\"smog\"] = 0\n",
    "        i[\"spache\"] = 0\n",
    "        try:\n",
    "            r = Readability(text_str)\n",
    "            sentences = sent_tokenize(text_str)\n",
    "            # Check if the sentence count is less than 30 for SMOG calculation\n",
    "            num_sentences = len(sentences)\n",
    "\n",
    "            # Tokenize words using nltk for flesch_kincaid()\n",
    "            wordlst = text_str.split()\n",
    "            num_words = len(wordlst)\n",
    "            if num_words >= 100:\n",
    "                i[\"flesch_kincaid\"] = r.flesch_kincaid().score\n",
    "                i[\"flesch\"] = r.flesch().score\n",
    "                i[\"fog\"] = r.gunning_fog().score\n",
    "                i[\"coleman_liau\"] = r.coleman_liau().score\n",
    "                i[\"dale_chall\"] = r.dale_chall().score\n",
    "                i[\"ari\"] =r.ari().score\n",
    "                i[\"linsear_write\"] =r.linsear_write().score\n",
    "                if num_sentences >= 30:\n",
    "                    i[\"smog\"] =r.smog().score \n",
    "                i[\"spache\"] =r.spache().score\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bfb02",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88252752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e9ac06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(listLine):\n",
    "    for i in listLine:\n",
    "        try:\n",
    "            blob = TextBlob(i[\"text\"])\n",
    "            i[\"polarity\"] = blob.sentiment.polarity\n",
    "        except:\n",
    "            i[\"polarity\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadd075",
   "metadata": {},
   "source": [
    "## VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebc91be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD = pd.read_csv('../NRC/NRC-VAD-Lexicon.txt', sep=\"\\t\", header=None)\n",
    "VAD.columns = [\"word\", \"valence\", \"arousal\", \"dominance\"]\n",
    "VAD_dict = VAD.set_index('word').T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f048924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_VAD(text, dim):\n",
    "    words_VAD = text.split()\n",
    "    score = [VAD_dict[i][dim] if i in VAD_dict else 0 for i in words_VAD]\n",
    "    return sum(score) / max(len(score), 1)\n",
    "\n",
    "def analyze_valence(text):\n",
    "    return emotion_VAD(text, 'valence')\n",
    "\n",
    "def analyze_arousal(text):\n",
    "    return emotion_VAD(text, 'arousal')\n",
    "\n",
    "def analyze_dominance(text):\n",
    "    return emotion_VAD(text, 'dominance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc5a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VAD(listLine):\n",
    "    for i in listLine:\n",
    "        i[\"valence\"] = analyze_valence(i[\"text\"])\n",
    "        i[\"arousal\"] = analyze_arousal(i[\"text\"])\n",
    "        i[\"dominance\"] = analyze_dominance(i[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d4c20",
   "metadata": {},
   "source": [
    "## Apply functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b7af0",
   "metadata": {},
   "source": [
    "applico qui tutte le funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMOJI\n",
    "extract_emojis(lines_def)\n",
    "#TIME HANDLING\n",
    "convert_time(lines_def)\n",
    "#CLEAN TEXT\n",
    "clean_single_text(lines_def)\n",
    "#CONTRACTIONS\n",
    "expand_contractions(lines_def)\n",
    "#UPPER WORDS\n",
    "unique_upper_words(lines_def)\n",
    "#PUNCTUATION\n",
    "remove_punct(lines_def)\n",
    "#FULL WORDS\n",
    "posTag(lines_def)\n",
    "#EMOTIONS\n",
    "get_emotion_vector(lines_def)\n",
    "#READABILITY\n",
    "readability_tests(lines_def)\n",
    "#TEXTBLOB POLARITY\n",
    "get_polarity(lines_def)\n",
    "#VAD LEXICON\n",
    "get_VAD(lines_def)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9723f7",
   "metadata": {},
   "source": [
    "## Turn into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fd5b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(lines_def, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55cf8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('PIANO_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
